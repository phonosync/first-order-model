{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pascal/opt/anaconda3/envs/first-order-model/lib/python3.7/site-packages/dask/config.py:129: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import imageio\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images):\n",
    "    fig = plt.figure(fig_size=(15, 15))\n",
    "    n_col = 4\n",
    "    n_row = math.ceil(len(images) %% n_col)\n",
    "    for i, im in enumerate(images):\n",
    "        plt.subplot(n_col, n_row, i+1)\n",
    "        plt.imshow(im)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quellen\n",
    "\n",
    "- Siarohin, A & Lathuilière, S & Tulyakov, S & Ricci, E & Sebe, N 2019 'First Order Motion Model for Image Animation', _Conference on Neural Information Processing Systems (NeurIPS)_, Dezember, 2019\n",
    "\n",
    "- Bulat, A & Tzimiropoulos, G 2017 'How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks', _International Conference on Computer Vision_, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "In einem ersten Schritt müssen wir unsere Daten vorbereiten. Dazu zählt das Video sowie das zu animierende Bild."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video\n",
    "\n",
    "Ein Video besteht aus mehreren Frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_video = \"video.mp4\"\n",
    "video = imageio.get_reader(path_video)\n",
    "\n",
    "frames = [video[i] for i in range(0,5,20)]\n",
    "plot_images(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Boxes\n",
    "\n",
    "Für unser Fake-Video konzentrieren wir uns nur auf das Gesicht. Daher wollen wir das Video entsprechend zuschneiden.\n",
    "Wir suchen in den einzelnen Frames nach dem Gesicht und zeichnen eine Bounding Box darum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False, device=device)\n",
    "\n",
    "def extract_bbox(frame, fa):\n",
    "    if max(frame.shape[0], frame.shape[1]) > 640:\n",
    "        scale_factor =  max(frame.shape[0], frame.shape[1]) / 640.0\n",
    "        frame = resize(frame, (int(frame.shape[0] / scale_factor), int(frame.shape[1] / scale_factor)))\n",
    "        frame = img_as_ubyte(frame)\n",
    "    else:\n",
    "        scale_factor = 1\n",
    "    frame = frame[..., :3]\n",
    "    bboxes = fa.face_detector.detect_from_image(frame[..., ::-1])\n",
    "    if len(bboxes) == 0:\n",
    "        return []\n",
    "    return np.array(bboxes)[:, :-1] * scale_factor\n",
    "\n",
    "for frame in frames:\n",
    "    bboxes = extract_bbox(frame, fa)\n",
    "    print(bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video zuschneiden\n",
    "\n",
    "Nachdem wir die Bounding-Boxen für jedes Frame gefunden haben, schneiden wir das Video entsprechend zu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zugeschnittenes Video\n",
    "Das neue Video sieht nur wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bild laden\n",
    "\n",
    "Neben dem Video benötigen wir auch ein Bild von Guy Parmelin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Fake\n",
    "\n",
    "Nachdem die Daten vorbereitet sind, können wir nun unser Fake Video erzeugen. Dies funktioniert mit zwei Modulen\n",
    "\n",
    "\n",
    "1. Modul zur Schätzung der Bewegung\n",
    "2. Modul zur Bilderzeugung\n",
    "\n",
    "\n",
    "### Modul zur Bewegungsabschätzung\n",
    "- Ein neuronales Netzwerk lernt in Bildern Schlüsselpunkte zu erkennen.\n",
    "\n",
    "-> Bild Schlüsselpunkte von treibendem Video sowie der Bilddatei\n",
    "\n",
    "- Wenn die Schlüsselpunkte aus den Videoframes mit dem Bild verglichen werden, können die Bewegungen abgeschätzt werden\n",
    "\n",
    "-> Bild einfügen als Beispiel\n",
    "\n",
    "- Die Bewegungen werden dann kodiert an das Modul zur Bilderzeugung übergeben\n",
    "\n",
    "\n",
    "### Modul zur Bilderzeugung\n",
    "\n",
    "- Das zweite Modul erzeugt das Bild. Dazu schätzt es ab, welche Bewegungen durch Verzerren des Bildes bewirkt werden können und wo das Bild \"übermalt\" werden soll\n",
    "- Anschliessend werden die angepassten Bilder wieder zu einem Video zusammengefügt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Ein neuronales Netzwerk erlernt wie es diese Aufgaben vollautomatisch erfüllen kann. Dieses Erlernen erfordert jedoch viel Rechenleistung und dauert lange.\n",
    "\n",
    "Deshalb verwenden wir in unserem Beispiel ein bereits trainiertes Netzwerk. Deshalb können wir direkt ein Fake-Video erzeugen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Videos zusammenfügen\n",
    "\n",
    "Damit wir das Resultat besser sehen, fügen wir das originale sowie das erzeugte Video zusammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weitere Möglichkeiten\n",
    "\n",
    "### Verschiedene Annimationen:\n",
    "\n",
    "![Screenshot](../sup-mat/vox-teaser.gif)\n",
    "\n",
    "![Screenshot](../sup-mat/fashion-teaser.gif)\n",
    "\n",
    "![Screenshot](../sup-mat/mgif-teaser.gif)\n",
    "\n",
    "\n",
    "### Face Swap\n",
    "\n",
    "![Screenshot](../sup-mat/face-swap.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}